{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined stopwords file\n",
    "# def append_file(source_file, destination_file):\n",
    "#     with open(source_file, 'r') as source, open(destination_file, 'a') as destination:\n",
    "#         for line in source:\n",
    "#             destination.write(line)\n",
    "\n",
    "# # Example usage\n",
    "# source_file = 'stopwords\\StopWords_Currencies.txt'  # Replace with the filename of the source text file\n",
    "# destination_file = 'stopwords\\StopWords.txt'  # Replace with the filename of the destination text file\n",
    "\n",
    "# append_file(source_file, destination_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14104"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a stop word list\n",
    "stop_list = []\n",
    "with open('stopwords\\StopWords.txt', 'r') as stop:\n",
    "    for i in stop:\n",
    "        stop_list.append(i.strip())\n",
    "\n",
    "len(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive words list\n",
    "pos_list = []\n",
    "with open('positive-words.txt', 'r') as pos:\n",
    "    for i in pos:\n",
    "        pos_list.append(i.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the output file\n",
    "output = pd.read_csv(\"Output Data Structure.xlsx - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_list = []\n",
    "with open('negative-words.txt', 'r', encoding=\"ISO-8859-1\") as neg:\n",
    "    for j in neg:\n",
    "        neg_list.append(j.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the 37.txt for sentiment analysis\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download the necessary NLTK data (only required for the first time)\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentence_tokens = sent_tokenize(text)\n",
    "\n",
    "    return word_tokens, sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 37.txt file usage\n",
    "output_words = []\n",
    "output_sen = []\n",
    "\n",
    "for i in range(37,151):\n",
    "    filename = f'content_files\\{i}.txt'  # Replace with the filename of the text file\n",
    "    word_tokens, sentence_tokens = tokenize_text(filename)\n",
    "    output_words.append(word_tokens)\n",
    "    output_sen.append(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 114)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_words), len(output_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the both tokens list in output df\n",
    "output['WORDS_TOKEN'] = output_words\n",
    "output['SEN_TOKEN'] = output_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 114 entries, 0 to 113\n",
      "Data columns (total 18 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   URL_ID                            114 non-null    int64  \n",
      " 1   URL                               114 non-null    object \n",
      " 2   POSITIVE SCORE                    0 non-null      float64\n",
      " 3   NEGATIVE SCORE                    0 non-null      float64\n",
      " 4   POLARITY SCORE                    0 non-null      float64\n",
      " 5   SUBJECTIVITY SCORE                0 non-null      float64\n",
      " 6   AVG SENTENCE LENGTH               0 non-null      float64\n",
      " 7   PERCENTAGE OF COMPLEX WORDS       0 non-null      float64\n",
      " 8   FOG INDEX                         0 non-null      float64\n",
      " 9   AVG NUMBER OF WORDS PER SENTENCE  0 non-null      float64\n",
      " 10  COMPLEX WORD COUNT                0 non-null      float64\n",
      " 11  WORD COUNT                        0 non-null      float64\n",
      " 12  SYLLABLE PER WORD                 0 non-null      float64\n",
      " 13  PERSONAL PRONOUNS                 0 non-null      float64\n",
      " 14  AVG WORD LENGTH                   0 non-null      float64\n",
      " 15  WORDS_TOKEN                       114 non-null    object \n",
      " 16  SEN TOKEN                         114 non-null    object \n",
      " 17  SEN_TOKEN                         114 non-null    object \n",
      "dtypes: float64(13), int64(1), object(4)\n",
      "memory usage: 16.2+ KB\n"
     ]
    }
   ],
   "source": [
    "output.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the stop words from the word tokens\n",
    "# final_words = [word for word in word_tokens if word not in stop_list]\n",
    "# len(final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\"“”’~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the punctuation\n",
    "punc = string.punctuation + '\"\"''“”’~'\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_words = [word for word in final_words if word not in punc]\n",
    "# len(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a positive score func\n",
    "def positive_score(words):\n",
    "    final_words = [word for word in words if word not in stop_list]\n",
    "    count = 0\n",
    "    for word in final_words:\n",
    "        if word in pos_list:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the positive score to the each rows of df\n",
    "output['POSITIVE SCORE'] = output['WORDS_TOKEN'].map(lambda x: positive_score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a negative score func\n",
    "def negative_score(words):\n",
    "    final_words = [word for word in words if word not in stop_list]\n",
    "    count = 0\n",
    "    for word in final_words:\n",
    "        if word in neg_list:\n",
    "            count += -1\n",
    "    return count*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the neg score to the each rows of df\n",
    "output['NEGATIVE SCORE'] = output['WORDS_TOKEN'].map(lambda x: negative_score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polarity score\n",
    "def polarity_score(pos_score, neg_score):\n",
    "    polarity = (pos_score - neg_score)/((pos_score + neg_score) + 0.000001)\n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate polarity score for each and every rows\n",
    "output['POLARITY SCORE'] = (output['POSITIVE SCORE'] - output['NEGATIVE SCORE'])/((output['POSITIVE SCORE'] + output['NEGATIVE SCORE']) + 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjectiviy score\n",
    "def subjectivity_score(pos_score, neg_score, words):\n",
    "    final_words = [word for word in words if word not in stop_list]\n",
    "    clean_words = [word for word in final_words if word not in punc]\n",
    "    sub = (pos_score + neg_score)/((len(clean_words)) + 0.000001)\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply subjectivity function to df\n",
    "output['SUBJECTIVITY SCORE'] = output.apply(lambda row: subjectivity_score(row['POSITIVE SCORE'], \n",
    "                                                                        row['NEGATIVE SCORE'],\n",
    "                                                                        row['WORDS_TOKEN']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the avg word per sentence\n",
    "def avg_word_len(word_token, sen_token):\n",
    "    words = [word for word in word_token if word not in punc]\n",
    "    avg_len = len(words)/len(sen_token)\n",
    "    return avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['SEN_TOKEN'] = output['SEN_TOKEN'].apply(lambda x: [1] if len(x) == 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply avg world len to the output df\n",
    "output['AVG NUMBER OF WORDS PER SENTENCE'] = output.apply(lambda row: avg_word_len(row['WORDS_TOKEN'],\n",
    "                                                                                    row['SEN_TOKEN']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the avg sentence length (which includes punctuation)\n",
    "def avg_sen(word_token, sen_token):\n",
    "    avg_words = len(word_token)/len(sen_token)\n",
    "    return avg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply thg avg_sen to the df\n",
    "output['AVG SENTENCE LENGTH'] = output.apply(lambda row: avg_sen(row['WORDS_TOKEN'],\n",
    "                                                                row['SEN_TOKEN']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count (after cleaning)\n",
    "def word_count(word_token):\n",
    "    final_word = [word for word in word_token if word not in stop_list]\n",
    "    clean_word = [word for word in final_word if word not in punc]\n",
    "    return len(clean_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applt the word_count func to the df\n",
    "output['WORD COUNT'] = output.apply(lambda row: word_count(row['WORDS_TOKEN']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syllable count \n",
    "import pyphen\n",
    "\n",
    "def count_syllables(word):\n",
    "    dic = pyphen.Pyphen(lang='en_US')\n",
    "    syllables = dic.inserted(word).count('-') + 1\n",
    "    return syllables\n",
    "\n",
    "def count_syllables_corpus(corpus):\n",
    "    syllable_count = 0\n",
    "    for word in corpus:\n",
    "        syllables = count_syllables(word)\n",
    "        syllable_count += syllables\n",
    "    return syllable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syllable count per word\n",
    "def syllable_count(words_list):\n",
    "    final_word = [str(word) for word in words_list if word not in stop_list]\n",
    "    clean_word = [str(word) for word in final_word if word not in punc]\n",
    "    syble_count = count_syllables_corpus(clean_word)\n",
    "    syble_word = syble_count/len(clean_word)\n",
    "    return syble_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['WORDS_TOKEN'] = output['WORDS_TOKEN'].apply(lambda x: [1] if len(x) == 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the syllable count func to the df\n",
    "output['SYLLABLE PER WORD'] = output.apply(lambda row: syllable_count(row['WORDS_TOKEN']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complex word count\n",
    "def com_syble(corpus):\n",
    "    compelx_words = 0\n",
    "    for word in corpus:\n",
    "        syllables = count_syllables(word)\n",
    "        if syllables > 2:\n",
    "            compelx_words += 1      \n",
    "    return compelx_words\n",
    "    \n",
    "\n",
    "def complex_word(words):\n",
    "    final_word = [str(word) for word in words if word not in stop_list]\n",
    "    clean_word = [str(word) for word in final_word if word not in punc]\n",
    "    com_count = com_syble(clean_word)\n",
    "    return com_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply complex word count func to the df\n",
    "output['COMPLEX WORD COUNT'] = output.apply(lambda row: complex_word(row['WORDS_TOKEN']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of complex words\n",
    "def per_compelx_word(words):\n",
    "    final_word = [str(word) for word in words if word not in stop_list]\n",
    "    clean_word = [str(word) for word in final_word if word not in punc]\n",
    "    com_count = com_syble(clean_word)\n",
    "    percentage = com_count/len(clean_word)\n",
    "    return round(round(percentage, 3)*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the complex words percentage func to the df\n",
    "output['PERCENTAGE OF COMPLEX WORDS'] = output.apply(lambda row: per_compelx_word(row['WORDS_TOKEN']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fog index\n",
    "def fog_index(words_lis, sen_lis):\n",
    "    len_words = len(words_lis)/len(sen_lis)\n",
    "    \n",
    "    final_word = [str(word) for word in words_lis if word not in stop_list]\n",
    "    clean_word = [str(word) for word in final_word if word not in punc]\n",
    "    com_count = com_syble(clean_word)\n",
    "    percentage = com_count/len(clean_word)\n",
    "    fog = 0.4*(len_words + percentage)\n",
    "    return fog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the fog index func to the df\n",
    "output['FOG INDEX'] = output.apply(lambda row: fog_index(row['WORDS_TOKEN'],row['SEN_TOKEN']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's count personal pronouns in text words\n",
    "import re\n",
    "\n",
    "def extract_personal_pronouns(words):\n",
    "    pattern = r'\\b(I|me|my|mine|we|us|our|ours|you|your|yours|he|him|his|she|her|hers|it|its|they|them|their|theirs|They|You|Our|My|He|She|We)\\b'\n",
    "    personal_pronouns = re.findall(pattern, ' '.join(words))\n",
    "    return len(personal_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['WORDS_TOKEN'] = output['WORDS_TOKEN'].apply(lambda x: ['a','b'] if len(x) == 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply personal pronouns func to the df\n",
    "output['PERSONAL PRONOUNS'] = output.apply(lambda row: extract_personal_pronouns(row['WORDS_TOKEN']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg word length\n",
    "def avg_word_char_len(words_list):\n",
    "    clean_word = [word for word in words_list if word not in punc]\n",
    "    len_char = \"\".join(clean_word)\n",
    "    count_len = len(len_char)\n",
    "    count_word = len(clean_word)\n",
    "    avg = count_len/count_word\n",
    "    return round(avg, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply avg word lenght to the df\n",
    "output['AVG WORD LENGTH'] = output.apply(lambda row: avg_word_char_len(row['WORDS_TOKEN']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-useful columns\n",
    "df_output = output.drop(['WORDS_TOKEN','SEN TOKEN','SEN_TOKEN'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as an Excel file\n",
    "df_output.to_excel('output.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcollab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
